{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "import glob\n",
    "import itertools\n",
    "import string\n",
    "#import spacy\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import random\n",
    "from tempfile import gettempdir\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words, n_words):\n",
    "    \"\"\"Process raw inputs into a dataset.\"\"\"\n",
    "    count = [['UNK', -1]]\n",
    "    count.extend(collections.Counter(words).most_common(n_words - 1))\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "    data = list()\n",
    "    unk_count = 0\n",
    "    for word in words:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0  # dictionary['UNK']\n",
    "            unk_count += 1\n",
    "        data.append(index)\n",
    "    count[0][1] = unk_count\n",
    "    reversed_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "    return data, count, dictionary, reversed_dictionary\n",
    "\n",
    "\n",
    "data_index = 0\n",
    "# generate batch data\n",
    "def generate_batch(data, batch_size, num_skips, skip_window):\n",
    "    global data_index\n",
    "    assert batch_size % num_skips == 0\n",
    "    assert num_skips <= 2 * skip_window\n",
    "    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "    context = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "    span = 2 * skip_window + 1  # [ skip_window input_word skip_window ]\n",
    "    buffer = collections.deque(maxlen=span)\n",
    "    for _ in range(span):\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    for i in range(batch_size // num_skips):\n",
    "        target = skip_window  # input word at the center of the buffer\n",
    "        targets_to_avoid = [skip_window]\n",
    "        for j in range(num_skips):\n",
    "            while target in targets_to_avoid:\n",
    "                target = random.randint(0, span - 1)\n",
    "            targets_to_avoid.append(target)\n",
    "            batch[i * num_skips + j] = buffer[skip_window]  # this is the input word\n",
    "            context[i * num_skips + j, 0] = buffer[target]  # these are the context words\n",
    "        buffer.append(data[data_index])\n",
    "        data_index = (data_index + 1) % len(data)\n",
    "    # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "    data_index = (data_index + len(data) - span) % len(data)\n",
    "    return batch, context\n",
    "\n",
    "####Helper fucntion to clean sentences####\n",
    "def clean_list_of_words(list_of_words):   \n",
    "    words = [word.lower() for word in list_of_words]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in words]\n",
    "    return stripped\n",
    "\n",
    "####Helper function to clean float to string####\n",
    "def int_or_float(s):\n",
    "    try:\n",
    "        return str(s)\n",
    "    except ValueError:\n",
    "        return float(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words (+UNK) [['UNK', 0], ('is', 519), ('for health', 316), ('is located in the', 267), ('is a', 224)]\n",
      "Sample data [362, 363, 1, 80, 41, 364, 98, 59, 41, 365] ['an', 'facial scar', 'is', 'acquired', 'nan', 'blue eyes', 'are', 'inherited', 'nan', 'long hair']\n"
     ]
    }
   ],
   "source": [
    "fpath = str(os.getcwd()) + '/Data/regents'\n",
    "all_files = glob.glob(fpath + \"/*.tsv\")\n",
    "list_of_dfs = [pd.read_csv(file, sep='\\t') for file in all_files]\n",
    "\n",
    "    \n",
    "#Create the header corpus\n",
    "#Collect the header items into a list \n",
    "#flatten the list\n",
    "#Remove header with Unnamed in it\n",
    "header_list=[]\n",
    "body_list = []\n",
    "cleaned_header_corpus = []\n",
    "cleaned_body_corpus = []\n",
    "\n",
    "for df in list_of_dfs:\n",
    "    \n",
    "    #####################Create the header corpus######################\n",
    "    header_list.append(df.columns.values)\n",
    "    flattened_header_list = [item for sublist in header_list for item in sublist]\n",
    "    flattened_header_list_clean = [item for item in flattened_header_list if not 'Unnamed' in item]\n",
    "    #print(flattened_header_list_clean) \n",
    "   \n",
    "    #Convert words into lower case\n",
    "    words = [word.lower() for word in flattened_header_list_clean]\n",
    "    #print(words[:100])\n",
    "    #Strip the punctuation from the table\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    cleaned_header_corpus.append([w.translate(table) for w in words]) \n",
    "    #print(cleaned_header_corpus[:100])\n",
    "    ################################################################## \n",
    "      \n",
    "    \n",
    "    #####################Create the body corpus######################\n",
    "    #Convert the body items to list\n",
    "    body_list = df.values.tolist()\n",
    "    #Flatten the listoflist to a list\n",
    "    flattened_body_list= [item for sublist in body_list for item in sublist] \n",
    "    #Convert floats in the list to int\n",
    "    flattened_body_strlist = [int_or_float(item) for item in flattened_body_list]\n",
    "    #Convert words to lowercase\n",
    "    body_words_lowerlist = [word.lower() for word in flattened_body_strlist]   \n",
    "    cleaned_body_corpus.append(clean_list_of_words(body_words_lowerlist))\n",
    "    \n",
    "    #print(cleaned_body_corpus[:100])\n",
    "    ##################################################################\n",
    "    \n",
    "#print(cleaned_body_corpus[:2])\n",
    "#print(cleaned_header_corpus)\n",
    "\n",
    "########Flattened Header and Body Corpus##############################\n",
    "merged_body_corpus = list(itertools.chain.from_iterable(cleaned_body_corpus))\n",
    "merged_header_corpus = list(itertools.chain.from_iterable(cleaned_header_corpus))\n",
    "\n",
    "\n",
    "vocabulary_size = 50000\n",
    "data, count, dictionary, reverse_dictionary = build_dataset(\n",
    "    merged_body_corpus, vocabulary_size) # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample data', data[:10], [reverse_dictionary[i] for i in data[:10]])\n",
    "\n",
    "data_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "363 facial scar -> 1 is\n",
      "363 facial scar -> 362 an\n",
      "1 is -> 80 acquired\n",
      "1 is -> 363 facial scar\n",
      "80 acquired -> 1 is\n",
      "80 acquired -> 41 nan\n",
      "41 nan -> 364 blue eyes\n",
      "41 nan -> 80 acquired\n"
     ]
    }
   ],
   "source": [
    "batch, labels = generate_batch(data,batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "    print(batch[i], reverse_dictionary[batch[i]], '->', labels[i, 0],\n",
    "        reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-e59e1d02f367>:24: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "    train_context = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    embeddings = tf.Variable(\n",
    "    tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "\n",
    "    # Construct the variables for the softmax\n",
    "    weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                              stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    \n",
    "    biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    hidden_out = tf.matmul(embed, tf.transpose(weights)) + biases\n",
    "    \n",
    "    \n",
    "    # convert train_context to a one-hot format\n",
    "    train_one_hot = tf.one_hot(train_context, vocabulary_size)\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hidden_out, \n",
    "        labels=train_one_hot))\n",
    "    # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(cross_entropy)\n",
    "    \n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
    "    normalized_embeddings = embeddings / norm\n",
    "    \n",
    "    valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "    \n",
    "    similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "    init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(graph, num_steps):\n",
    "    with tf.Session(graph=graph) as session:\n",
    "      # We must initialize all variables before we use them.\n",
    "        init.run()\n",
    "        print('Initialized')\n",
    "\n",
    "        average_loss = 0\n",
    "        for step in range(num_steps):\n",
    "            batch_inputs, batch_context = generate_batch(data,\n",
    "                batch_size, num_skips, skip_window)\n",
    "            feed_dict = {train_inputs: batch_inputs, train_context: batch_context}\n",
    "\n",
    "        # We perform one update step by evaluating the optimizer op (including it\n",
    "        # in the list of returned values for session.run()\n",
    "        _, loss_val = session.run([optimizer, cross_entropy], feed_dict=feed_dict)\n",
    "        average_loss += loss_val\n",
    "        \n",
    "        print(average_loss)\n",
    "        if step % 2000 == 0:\n",
    "            if step > 0:\n",
    "                average_loss /= 2000\n",
    "                # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "            print('Average loss at step ', step, ': ', average_loss)\n",
    "            average_loss = 0\n",
    "            \n",
    "            \n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "        if step % 10000 == 0:\n",
    "            sim = similarity.eval()\n",
    "            for i in range(valid_size):\n",
    "                valid_word = reverse_dictionary[valid_examples[i]]\n",
    "                top_k = 8  # number of nearest neighbors\n",
    "                nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "                log_str = 'Nearest to %s:' % valid_word\n",
    "                for k in range(top_k):\n",
    "                    close_word = reverse_dictionary[nearest[k]]\n",
    "                    log_str = '%s %s,' % (log_str, close_word)\n",
    "                print(log_str)\n",
    "        final_embeddings = normalized_embeddings.eval()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "11.09147834777832\n",
      "Softmax method took 0.517762 minutes to run 100 iterations\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100\n",
    "softmax_start_time = dt.datetime.now()\n",
    "run(graph, num_steps=num_steps)\n",
    "softmax_end_time = dt.datetime.now()\n",
    "print(\"Softmax method took {} minutes to run 100 iterations\".format((softmax_end_time-softmax_start_time).total_seconds()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
